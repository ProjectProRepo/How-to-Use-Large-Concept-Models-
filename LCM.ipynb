{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH0DrFvr/59SHN33LHCZzS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fairseq2==v0.3.0rc1 --pre --extra-index-url  https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cu124 --upgrade\n",
        "!pip install -q sonar-space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3GvZItCTd6l",
        "outputId": "50d6e5ff-8325-4d41-c874-0bef30080be5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sonar-space 0.4.0 requires fairseq2~=0.4.0, but you have fairseq2 0.3.0rc1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pPWL4EVxTNJD"
      },
      "outputs": [],
      "source": [
        "!pip install -q  wtpsplit sonar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVfMGW0STQle",
        "outputId": "50d8b9c8-ad35-4ff3-c2ab-3fc4f2114d83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from wtpsplit import SaT\n",
        "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
        "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline"
      ],
      "metadata": {
        "id": "GY6KD_Kcd0gk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embd_dim, dim, layers, heads, dropout, device):\n",
        "        super().__init__()\n",
        "        self.embd_dim = embd_dim\n",
        "        self.dim = dim\n",
        "        self.layers = layers\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.prenet = nn.Sequential(\n",
        "            nn.LayerNorm(embd_dim),\n",
        "            nn.Linear(embd_dim, dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)  # Dropout to prevent overfitting\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model=dim, nhead=heads, dropout=dropout) for _ in range(layers)\n",
        "        ])\n",
        "\n",
        "        self.postnet = nn.Sequential(\n",
        "            nn.Linear(dim, embd_dim),\n",
        "            nn.Softmax(dim=-1)  # Softmax to ensure valid probability distribution\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prenet(x)\n",
        "        for l in self.decoder:\n",
        "            x = l(x, x)\n",
        "        return self.postnet(x)"
      ],
      "metadata": {
        "id": "OXfGqe9pj1Rl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LCM Model\n",
        "class LCMModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.sat_sm = SaT(config.model_name)\n",
        "        print(\"Splitter initialized\")\n",
        "\n",
        "        self.t2vec_model = TextToEmbeddingModelPipeline(\n",
        "            encoder=config.sonar_enc, tokenizer=config.sonar_enc, device=torch.device(config.device)\n",
        "        )\n",
        "        print(\"Text-to-Vector model initialized\")\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            config.embd_dim, config.dim, config.layers, config.heads, config.dropout, config.device\n",
        "        ).to(config.device)\n",
        "        print(\"Transformer initialized\")\n",
        "\n",
        "        self.vec2text_model = EmbeddingToTextModelPipeline(\n",
        "            decoder=config.sonar_dec, tokenizer=config.sonar_dec, device=torch.device(config.device)\n",
        "        )\n",
        "        print(\"Vector-to-Text model initialized\")\n",
        "\n",
        "    def split_into_concepts(self, text):\n",
        "        return self.sat_sm.split(text, threshold=self.config.threshold)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        out_embeddings = self.transformer.forward(embeddings)\n",
        "        return out_embeddings\n",
        "\n",
        "    def generate(self, text, num_generated_concepts=1):\n",
        "        with torch.no_grad():\n",
        "            concepts = self.split_into_concepts(text)\n",
        "            print(\"\\nInitial Concepts:\", concepts)  # Debugging\n",
        "\n",
        "            for c in range(num_generated_concepts):\n",
        "                embeddings = self.t2vec_model.predict(concepts, source_lang=self.config.lang)\n",
        "                print(\"\\nEmbeddings:\", embeddings)  # Debugging\n",
        "\n",
        "                out_embeddings = self.forward(embeddings)\n",
        "                print(\"\\nTransformed Embeddings:\", out_embeddings)  # Debugging\n",
        "\n",
        "                # Removed 'num_beams' to prevent TypeError\n",
        "                next_concept = self.vec2text_model.predict(\n",
        "                    out_embeddings, target_lang=self.config.lang, max_seq_len=self.config.max_seq_len\n",
        "                )\n",
        "                print(\"\\nGenerated Concept:\", next_concept)  # Debugging\n",
        "\n",
        "                concepts.append(next_concept[0])\n",
        "\n",
        "        return \" \".join(concepts)  # Return as a proper sentence\n"
      ],
      "metadata": {
        "id": "AvFe3nxbjxCx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Class\n",
        "class LCMConfig:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # Transformer args\n",
        "        self.embd_dim = 1024  # Dimension of SONAR embeddings\n",
        "        self.dim = 1024       # Keep this close to embedding size\n",
        "        self.layers = 2       # Reduce layers for better optimization\n",
        "        self.heads = 8        # Number of attention heads\n",
        "        self.dropout = 0.1    # Add dropout to prevent overfitting\n",
        "\n",
        "        # Sonar args\n",
        "        self.lang = \"eng_Latn\"\n",
        "        self.max_seq_len = 256\n",
        "        self.sonar_enc = \"text_sonar_basic_encoder\"\n",
        "        self.sonar_dec = \"text_sonar_basic_decoder\"\n",
        "\n",
        "        # wtpsplit args\n",
        "        self.model_name = \"sat-1l-sm\"\n",
        "        self.threshold = 0.05"
      ],
      "metadata": {
        "id": "KEOpkG4FiBGu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and Run\n",
        "config = LCMConfig()\n",
        "lcm = LCMModel(config)\n",
        "\n",
        "text = \"This is a test sentence.\"\n",
        "output = lcm.generate(text, num_generated_concepts=2)\n",
        "print(\"\\nGenerated Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voBfNTWfjrKa",
        "outputId": "34623723-20ec-43c3-83b2-0efbdb2ad7b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitter initialized\n",
            "Text-to-Vector model initialized\n",
            "Transformer initialized\n",
            "Vector-to-Text model initialized\n",
            "\n",
            "Initial Concepts: ['This is a test sentence.']\n",
            "\n",
            "Embeddings: tensor([[ 0.0013, -0.0023, -0.0098,  ..., -0.0089,  0.0024, -0.0056]],\n",
            "       device='cuda:0')\n",
            "\n",
            "Transformed Embeddings: tensor([[0.0006, 0.0008, 0.0005,  ..., 0.0005, 0.0009, 0.0023]],\n",
            "       device='cuda:0')\n",
            "\n",
            "Generated Concept: [\"In the meantime, I'm going to share with you some of the things that I've learned from the past, and I'm going to share with you some of the things that I've learned from the past.\"]\n",
            "\n",
            "Embeddings: tensor([[ 0.0013, -0.0023, -0.0098,  ..., -0.0089,  0.0024, -0.0056],\n",
            "        [-0.0080,  0.0067,  0.0058,  ..., -0.0055, -0.0006,  0.0126]],\n",
            "       device='cuda:0')\n",
            "\n",
            "Transformed Embeddings: tensor([[0.0010, 0.0015, 0.0006,  ..., 0.0005, 0.0010, 0.0016],\n",
            "        [0.0013, 0.0009, 0.0012,  ..., 0.0011, 0.0016, 0.0027]],\n",
            "       device='cuda:0')\n",
            "\n",
            "Generated Concept: ['In the meantime, I\\'m going to share with you some of my favorite quotes from the book: \"With the help of God\\'s Spirit, we are able to make the most of the time we have.', \"In the meantime, I'm going to take a look at some of the ways in which I've been able to do this: I'm going to take a look at some of the ways in which I have been able to do this.\"]\n",
            "\n",
            "Generated Output: This is a test sentence. In the meantime, I'm going to share with you some of the things that I've learned from the past, and I'm going to share with you some of the things that I've learned from the past. In the meantime, I'm going to share with you some of my favorite quotes from the book: \"With the help of God's Spirit, we are able to make the most of the time we have.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsTTFq7ak_vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}